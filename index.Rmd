--- 
title: "Data Science Within Monitoring and Evaluation in Humanitarian Context"
author: "Cagri Cebisli"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---

bookdown::render_book("index.Rmd")

# Introduction

This book is tailored for revealing key concepts of intersections between "data science" and "monitoring and evaluation" for humanitarian sector. Do not question the language since most of these thoughts and conclusions are personal, and this book acts as a notebook of my experiences, also, I am horrible at writing stuff down. No shame, that is the golden rule of being monitoring and evaluation person. Shame triggers emotions, and as outcome one hides stuff and reflects only bright side, and that reduces lessons learned capacity for sure.

Five years in humanitarian sector as data scientist with background of industrial engineering and management information systems, I have encountered gaps and common mistakes in the sector. For some parts, I will be questioning these gaps and unknowns for building a collective knowledge, hoping that it will help the future actions. Through the book I will take a guess that reader have basic knowledge of monitoring and evaluation in humanitarian sector so I will dodge going in-depth of the aspect, and the book will focus on data analysis and tips&tricks of designing operational planning.

Humanitarian sector actors has background of "monitoring and evaluation". There are many individuals whom got mindset and experiences towards this aspect. Yet, in the latest versions of M&E, inevitably this aspects merges with data and knowledge. One of the most experienced individuals with humanitarian data Aldo Benini reflects in his notes; "Keep it simple. You may use R or Python, but if you want to be a voice for this spectacular community, stay with excel, since most of the people are more relaxed using this." This conclusion of years of experience is still valid. Yet, I am observing two types of individuals. On one hand you have mid-class proficient excel users, whom can conduct (descriptive) analysis. On the other hand one has very skilled individuals whom can use R or Python with their amazing machine learning skills and so on. Thus, in the sector, we do not have one common ground but two.

As for data analysis for humanitarian sector, with the motivation of donors, we are observing more "evidence-based" actions. One of the old-school WFP manager once told in a VAM training: "Back than we were disturbing in-kinds as food, hygiene whatsoever, and then come back home. Nobody was asking question neither we did care for it. We believed we did good and that's all. But now, there is data everywhere and logic is more stronger." This is a clear vision of experience, tolds us how data become more important during years.

For red pillar, IFRC is pushing more for IM - when they are losing sight of M&E analysts and mark this concept as PMER where these individuals mostly have no idea about data analysis. So it looks like quantitative staff is IM and qualitative staff is PMER, which is a terrible idea. A data scientist can become IM, but can not become M&E data analyst, since they have to have an experience for programme design-logframes-impact analysis and understanding concept of the programme. Frankly, we did have an issues with this mindset while working with IFRC. M&E Data Scientist and IM persons are not the same thing. Through the book, one will realize this more. ICRC does cover a different path which I would prefer as well. They call this phenomenon as "Analysis and Evidence Team". Which does serve their structure well, as they are more enveloped and their work-groups are focusing on very different topics. Yet, they have intersections such as between Protection(unit) - ECOSEC or ECOSEC (economic security) - WATHAB (water habitat). But mentality of the unit is the same. It is like having a team that makes knowledge out of ones data or any secondary data. That team creates room for being more agile, moving with evidence, not with instincts (tho sometimes it's better, we do not make data fetish here). Their team can be interpreted as M&E Data Analyst team, under covered. If you read their documents, they are pretty similar with any other M&E guidelines and handbooks. For some programme coordinators (usually happens in National Societies) I am seeing the attitude of taking data analysis, evidence-based decision making and M&E is a layer of quality of a programme instead of considering M&E as core foundation of any humanitarian programming which should shape other operational/programmatic components. This mindset is an eclipse in humanitarian sector. These thoughts are belongs to past, what WFP manager refers to. This eclipsed-mind still lives in 50 years ago, where only aim of a programme is to deliver assistance, without any monitoring, without understanding needs, without any targeting, without any reporting at all. If you see an environment and attitude like that, just leave the ship. Donors will not approve that either.

For someone who worked for ESSN, biggest cash programme of ECHO and in humanitarian sector, I have an understanding of how ECHO interprets monitoring and what they expect out of evaluation studies. ECHO does care about evidence-based management style, and M&E. They do respect their logframes. Thus, if one puts an indicator there can expect a set of questions towards its interpretation. Also in interim reports they are very interested in calculation of MEB, how indicators talks with each other and so on. So if you are an M&E data analyst with ECHO supported programme, expect series of questions and prepare good story-telling 3-pagers. It will be a challenge! I strongly recommend to read out DG Echo Thematic Polict Document No 3 - Cash Transfers, March 2022.

Personally, I am not experienced with blue pillar (UN). Yet WFP is leading this concept and they are disaggregating more into "division of labor" doctrine. They have monitoring unit, sided by VAM unit (vulnerability analysis and mapping). Even there is a unit collaborated by UNHCR and WFP, just for targeting. Also they have an unit studies targeting. Worked with them for targeting exercises for camps, and I learned a lot from them! No wonder why they are in a leading role, as they care and dig more into these lands. Most of the red-pillar managers does not understand what targeting is, yet other hand has an unit for it. As mentioned, personally, I do have very little information of other UN institutions. Yet, WFP is a shining role-model for sure.

This is a one, dusty overlook to humanitarian sector and data. I am smelling a trend between cash based interventions and data-knowledge tree. Huge correlation is lurking, if you check ECHOs latest Large-scale cash programme guidelines, lots on monitoring and evaluation explained and M&E without data is useless. Thus, if humanitarian sector evolves more into large-scale cash, data and M&E will become more and more important. In this book, I will not dig into if it is possible to do more cash-based interventions and infrastructure of banks or stakeholders' potential. Yet we all can agree that cash still has a big future and a room in our daily lives as humanitarians and Ukraine crisis proves that right. So lets build more towards data analysis and M&E to increase accountability, evidence-based actions and to have better programmes.Note that in this paper, I am assuming that reader has basic understanding of M&E, and stresses more on the data dimension.

##    Audiance

The primary audience for this book is data science / data analyst practitioners whom works for monitoring and evaluation units, from organisations directly involved in the design, implementation, monitoring, and accountability of projects using cash and vouchers to deliver humanitarian relief. For example:

-    Monitoring and evaluation data analysts
-    PMER staff with data skills
-    IM teams involved in humanitarian data analysis
-    MEAL specialists etc.

The secondary intended audience is other humanitarian stakeholders involved in advancing CTP monitoring and evaluation practices. Thus, may include TPM institutions.

##    Why Is This a Book Needed?

As mentioned in introduction, data analysis within monitoring and evaluation is a growing aspect. Many new colleagues are taking part, some of them has skills towards data analysis and grades up from field teams, some are just new to the sector or to data analysis. In this book, I will mention about M&E aspect and give some examples through operation, data cleaning, data analysis and basic examples of calculating humanitarian indicators (few important ones usually involves with cash-based interventions) with R. Thus, my belief is to make this book an honest starter for my colleagues, to read and learn from each other. Having no other hands-on guidance and written experiences was missing for me, so I am trying to fill that gap. Of course, every institution has their SOPs, but these SOPs are usually serves and limited with their current context.

##    Book Structure

This book mainly runs in two pillars; i) Monitoring and evaluation design in humanitarian sector ii) Data Science methods. In this book, for every section, I will set a scenario, a scene to reflect reality and then will take action with coding and explain how to implement studies to a real life situation. 


Data sets will be random, the ones comes with packages. Anyhow, for different exercises I will be using random data sets from some packages but main concept will be same with humanitarian data. Here, mentality of tidy data comes into play, where every row stands for analysis unit and every column stands for variables. 


![Structure of the Book](C:\R Working Directory\R Bookdown\HumanitarianBook1\BookStructure.JPG)




#    Humanitarian Projects and Monitoring and Evaluation

Before moving forward, a basic set of project-monitoring definitions could be useful. There are several types of monitoring in any programme such as; process monitoring, results monitoring, context monitoring, risk monitoring etc. Monitoring4CTP, a monitoring guidance for CTP made by USAid and CALP, is a fruitful and well-tailored document. I strongly advise this document to anyone who works in humanitarian sector.

![Project Monitoring by CALP](C:\R Working Directory\R Bookdown\HumanitarianBook1\Project Monitoring.JPG)

As per monitoring type, there are several method intersections. Yet, the data skills comes into play when we ask evaluation questions. An amazing visual prepared by IFRC, we can clearly see those steps and questions. 

![Project Monitoring by CALP](C:\R Working Directory\R Bookdown\HumanitarianBook1\Evaluation and Logframe.JPG)

Yes, all good, but why M&E is important ?

-   Support project/programme implementation with accurate, evidence-based reporting that informs management and decision-making to guide and improve project/programme performance.

-   Contribute to organizational learning and knowledge sharing by reflecting upon and sharing experiences and lessons so that we can gain the full benefit from what we do and how we do it.

-   Uphold accountability and compliance by demonstrating whether or not our work has been carried out as agreed and in compliance with established standards.

-   Provide opportunities for stakeholder feedback, especially beneficiaries, to provide input into and perceptions of our work, modelling openness to criticism, and willingness to learn from experiences and to adapt to changing needs.

- Promote and celebrate our work by highlighting our accomplishments and achievements, building morale and contributing to resource mobilization

These points are quoted from M&E guide of IFRC. All smooth right? Did you ever see a humanitarian programme, especially a cash-based intervention with zero monitoring? I did and it was horrible. No accountability, no promotion and celebration of work, no highlights, no lessons learned, no voice of beneficiaries. Just distribute some cash to undefined and unjustified target group and come back home. Still think a programme without M&E or AAP/CEA is a good idea? Still think that this is a quality layer and not a must? You will not survive in humanitarian sector. Imagine you are a donor, would give your money to someone with good M&E system and monitoring plan or would you pick other way around. 

#   Research Design for Monitoring and Evaluation Studies

In humanitarian sector research design is actually links with monitoring and evaluation plan of the programme. Through all different style of M&E plans (actually can be called as research design of making programmes accountable and measurable), there are several topics that I see as a must.

-   An introduction
    -   What is happening in context and objective of the programme
    -   If secondary data available, most changing elements like a brief summary of markets etc.
-   Purpose of the plan
    -   By implementing this plan, which questions we will answer? (Is programme impactful as intended? could be a research question for M&E)
    -   Why we are doing M&E and where are the bottlenecks
    -   Expected outcomes and reporting
-   Description of the process, which activities will be implement?
    -   Example of activities; FGD, PDMs, KIIs, Needs Assessments etc.
-   Scope of each activity (disaggregated by activity)
    -   How data will collect? What will be modality of data collection.
    -   Requirements on the field. Tools needed to conduct activities.
    -   How many man/work hours do we need?
    -   What will be the frequency of reports and what/how to report?
-   Data flows
    -   What is the data collection tool, KOBO/ODK etc?
    -   Where is the database?
    -   Data security
    
##    Logframes, Indicators and Questionnaire Design

Through logframe, there will be several indicators that is measured by M&E teams. Team must tailor surveys to collect these indicators. There are several different methods of reflecting a log frame, yet, the one I propose is below. This version reflects anything needed; a description of indicator, data source, baseline, target, data source, frequency, main responsible, and most importantly, assumption and risks.

![Logframe overlook, kindly zoom in to read, sorry!](C:\R Working Directory\R Bookdown\HumanitarianBook1\Logframe Indicators.JPG)

Lets say, you are an M&E specialist to a humanitarian programme. Donor asks you to use SMART indicators (please do google it, very useful topic). Then, with the blessings of donor, or in your proposal to a donor, you reflect that your indicators will be rCSI and FCS. Second step is to designing your PDMs. rCSI is an indicator that builds over 5 questions. Thus, you have to have these questions to calculate and report rCSI. We will dig more into these indicators, how to calculate them with R in the following sessions. In a nutshell, questionnaire design orients around logframe indicators. Simple as that. 

![Calculation of rCSI](C:\R Working Directory\R Bookdown\HumanitarianBook1\rCSI Indicator.JPG)

Additional parts of questionnaire design comes from analyst. If analyst wants to conduct a vulnerability study depending on multi-dimensional aspect, that includes household assets, these can be added as wished. So any metric compounds of set of questions can be added to the questionnaire. There are two things analyst must be careful about;

-   Data collection modality.
    -   If modality is via phone, surveys must be short, respecting data quality.
    -   Questions must be simple and easy to collect via phone.
    -   Even for the face to face surveys, length of questionnaire is an important aspect.
    -   Communication must be clear and questions must not be complicated or open to bias.
    
-   Every question must serve to a purpose. Avoid unnecessary questions or any open-ended ones for quantitative study.

##    Data Collection Tools

There are several different tools to collect data. IFRC generally uses KOBO. If you google, there are two links for KOBO. IFRC one called https://kobonew.ifrc.org/ and other one is https://www.kobotoolbox.org/. As far as I am aware Kobotoolbox is under OCHA's administration. But why we have two different KOBO? That is because of servers. IFRC servers in Frankfurt, OCHA based in US. Thus, your data copyrights are protected by those laws.

https://humanitarian.atlassian.net/wiki/spaces/imtoolbox/pages/3190489103/Kobo+Toolbox+Terms+of+Service:


U.S. DIGITAL MILLENNIUM COPYRIGHT ACT.
If UNOCHA receives a notice alleging that material from your account infringes another party’s copyright, UNOCHA may disable your user account or remove alleged material in accordance with Title II of the Digital Millennium Copyright Act of 1998 (Section 512 of the U.S. Copyright Act).

ICRC uses "Device Magic" and they have their own servers. Turkish Red Crescent uses different elements, in ESSN they are using ODK and servers are based in Turkey (admin is Turkish Red Crescent) due to data security. 

So there are several different means of data collection tools. My perspective, I find KOBO very efficient, but there is always a question of data security if you don't own the server. 

In this book, I will dodge how to use those platforms. Yet, they are pretty simple and KOBO even has UI to do it. Took me two nights to completely understand how it works. Also, there are several training available online.

##    Sample

**Taken from "IndiKit Rapid Guide to Survey Sampling", an amazing guide for humanitarians.**

Sample is basically choosing who to survey. First, define your target population – the people we are interested in collecting data from.

The most common target populations are:

-   people whom your intervention aims to help (its direct beneficiaries)
-   other people who might have benefited indirectly (can be used for assessing the intervention’s spillover effect)
-   other stakeholders of your interventions whose opinions and other data you need to assess
-   members of your comparison group 
-   various population groups living in a given area (commonly used for needs assessments)

Keep in mind that one survey can have several different target populations. For example, a baseline survey of a nutrition-sensitive agricultural intervention might target children under 5 years (for measuring the prevalence of under nutrition), their caregivers, trained farmers as well as agriculture extension workers. Such surveys then require separate samples for each target group.

Please kindly refer to document mentioned above. Here, I will be giving some tips and tricks from my personal experiences and try to reflect how I do things operationally, in a most simplified way.

![Sampling Roadmap](C:\R Working Directory\R Bookdown\HumanitarianBook1\Sampling.JPG)

###   Analysis Unit

While building logframe, indicators are mostly orients around household level. Like copings, food security, expenditures etc. Thus, most of the time in humanitarian projects "analysis unit" is households. Some cases could be individual such as disability projects, education etc. Your target group defines perspective of the project. In a programme, if your registration is at household level, automatically analysis unit will be in household.

###   Non-Response Rates

Non-response rates are very important. This means a sample unit from your list that operators or FMAs cannot reach. If you have to reach 385 households in a region and have a list of 400, with %50 non-response rate, your list will not be enough for representative number. You need at least 385 x 2 = 770 sample to reach desired number of surveys.

###   Camp Context

In camp context, most easy-to-apply method is following;

-   Give numbers to each containers and convert it to a vector and shuffle it.
-   Generate random numbers from that vector by order.
-   Pick intersection of two vector as sample.

There you have it. A random selection by containers for sampling. You may need satellite image to see the number of containers and where they are settled. If approved, a drone may do as well. 

Attention is required for analysis unit-container relationship. If your analysis unit is households, than your sampling unit must be as well. Yet, imagine a concept where containers are not obtained by per household. Than, different set of approach is required. Thus, effecting questionnaire design.

###   Drawing Sample with R

####    Layered-Clustered Sample

In humanitarian sector, I see that programme monitors usually applies clustered samples. Cluster layers could be regions, nationality, gender etc. where researchers wants to have a representative knowledge for each cluster group. There are several different methods for sampling. I personally like machine learning style of sampling. 


```{r}
library(tidyverse)
library(ggplot2)
head(diamonds,5)
```

Lets do this exercise over "Diamonds" data comes with tidyverse package. Thus, this data is population data and you need to draw some samples for a study.

**Setting the Scene:**

You are a data scientist works with monitoring and evaluation unit in country office x. There is a cash-based intervention and you are responsible for designing a PDM. In programme objective, it says; increasing economic situation of the target population. So this is basically your research question. Now it is time to build a research design. In secondary data review, you find out that for specific regions, socio-economical indicators are very different (livelihoods and expenditures etc.). Thus, to have an accurate outcomes, you design a representative sample for each region, to have a better understanding of the concept. Operationally, you have enough capacity to collect data within promised time (I am stating that because dividing sample into layers means more surveys, it could be overall 385 or for two regions 385 x 2). Now, you have the population data and sample stage is at hand. In first version, we will draw a quick sample from overall population and then move to more simplified version of layered sample.


```{r}
#A quick overlook to data
glimpse(diamonds)
```

Lets take "cut" as region here. This variable is categorical with 5 unique values. There are several good-ends of taking sample with data partition. More info can be found in: **http://topepo.github.io/caret/data-splitting.html**

If you need to calculate threshold (desired number for representative sample), **Raosoft (http://www.raosoft.com/samplesize.html)** is a good sample size calculator. So in this example, lets decide metrics wih Raosoft; with %5 margin of error, %95 confidence level, population size of 53940 and %50 response distribution (maximum unknown) our sample size is 382. Including %50 non-response rate sample size (a call list or interviewee list) is 764. In the current situation we are using cluster sampling so that for five different categories (could be region, nationality etc. in humanitarian sector) we need to draw 382 (764) for each of them. We did take 'cut' as our layer in the sampling. Thus, for five categories we need sample of 764 * 5 = 3820 sample which is ~%7.1 of our population.

```{r}
#Easiest way to draw layered sample. One row coding, amazing package!

layered_sample <- diamonds %>% group_by(cut) %>% slice_sample(n=780,replace = FALSE)
table(layered_sample$cut)
```


Now that we draw sample. But, we have to compare population metrics with sample metrics. As stressed in scenario paragraph, we are trying to understand socio-economic variables from sample, and then we will assume these findings applies to whole population. 

A small trick, we might forecast how successful our sampling in terms of making conclusions. We do have an information about our sample; lets say household size. So if your sample household size distribution and your population is not statistically different, we can assume that our sample does well and outcomes of socio-economic conclusions are applies to whole population.

Lets start with categorical variables. Below, from graphs, one can see that distribution of clarity and type (Remember these are coming from diamonds data. You can interpret them as any other categorical variable that is available in humanitarian sector). What this visual tells us is this: density of group taken into consideration and distributions applies to whole case, without leaving any different characteristic behind. So that is a good sample!

```{r}
diamonds$Type <- 'Population'
layered_sample$Type <- 'Sampled'
Compare <- rbind(layered_sample,diamonds)

ggplot(Compare, aes(clarity, Type)) +
  geom_jitter(aes(color = clarity), size = 0.3)+
  ggpubr::color_palette("jco")+
  ggpubr::theme_pubclean()
```

Let's double check with numeric variables and compare them by type. Depth and Table variables taken into consider. Check how both linear models does not aligns with each other perfectly, but does well enough. Here, we can assume that our sample does really well!

```{r,warning=FALSE,error=FALSE,message=FALSE}
library(hrbrthemes)
Compare %>% 
ggplot(aes(x = depth, y = table, group = Type, color = Type)) +
        geom_point() +
        geom_smooth(method = "lm", se = FALSE) +
        labs(x = "Variable Table",
             y = "Variable depth") +    
theme_ipsum()
```

Key differences between sample and population happens due to we give same importance to each cut category. But in reality, their distribution (their frequency-density in population) is different, yet we did draw 780 sample from each of them. This is where one must apply weights in order to balance the outcomes and to have better accuracy while making assumptions of population values. Weights can be calculated as: each categories' sample size ratio (# of survey of each category divided by total sample-survey number) divided by each categories' population size ratio (frequency of each category divided by total population number) will give you weights. 

**PUT WEIGHT TABLE HERE**

####    Random Sampling

For random sampling, without any clusters, data partition technique is amazing. Lets say we want to draw 3820 sample out of 53940 (row number of diamonds data), which is ~7.1% of our total population. "p" metric in data goes as 0.71. We mark selected cases as index, and split data by sampled and outsampled. Distribution here will be more accurate then layered sampling. Please do check linear line and table of cut variable.


```{r}
#Select rows - proportionally depending on "cut"~ acts as region.
#library caret contains "createDataPartition" function.
library(caret)

diamonds_index <- createDataPartition(diamonds$cut, p = .071, 
                                  list = FALSE,
                                  times = 1)

#Divide data by sample: selected -> sampled & Not in sample -> outsample
diamonds_sample <- diamonds[ diamonds_index,]
diamonds_outsample  <- diamonds[-diamonds_index,]
```

```{r}
#As you see, frequency of cut category is not same with clustered version.
#Percentage of each cut category is same with population percentages now.
table(diamonds_sample$cut)
```

```{r,warning=FALSE,error=FALSE,message=FALSE}
library(hrbrthemes)

diamonds$Type <- 'Population'
diamonds_sample$Type <- 'Sampled'
Compare_partition <- rbind(diamonds_sample,diamonds)

Compare_partition %>% 
ggplot(aes(x = depth, y = table, group = Type, color = Type)) +
        geom_point() +
        geom_smooth(method = "lm", se = FALSE) +
        labs(x = "Variable Table",
             y = "Variable depth") +    
theme_ipsum()
```

You can see that linear models are overlaps better, if you compare it to clustered sample. This is also another good example of why we need to apply weights in clustered samples, before making an assumption towards population.

####    Lessons Learned from Sampling

I would also like to reflect few lessons learned from my end towards sampling-operational balance.

-   If you cluster your sample with regions, select proper enumerator for proper regions. In some cases, some regions may speak different language, or attendees may be more coherent when enumerator speaks their language or when enumerators are from same region.

-   Selecting an enumerator from or experience with that region usually reduces survey time and increases data quality. Also they are better when you ask for feedback towards questionnaire.

-   Take into consideration of gender of your enumerator, where in some regions female enumerators can have positive added values while surveying.

-   Keep in touch with enumerators, if they feel something is wrong with sampling, let them warn you. For ex; all the attendees saying that they are living in some other region than your sampling list-call list.

-   Keep track of enumerators if you have clustered sample. Lets say you are calculating %50 of non-response rate of 300 survey, thus your sampling is 600. For some clusters non response rate might be lower, so they may exceed the number of 300, means that you have to stop those enumerators to do extra calls. But of course, always have some extra calls since you might delete some records during data cleaning. And vice-versa, some clusters may have non response rate more than %50, so you might need to give them extra call list during operation.

-   When you share your coding, do not forget to add seeds into your coding, so that your studies can be reproducible by others.

-   When you do random sampling over population data, if you do care a bit about some categories as well (like region, sex, nationality etc.), their frequency might be low like 30~ish in your sample. Round them up to at least 50-60 to get an idea about those disaggregations so that you can conduct some t-test etc.

This marks the end of our sampling section. Always remember, we need good sampling to make accurate decisions. There is a chance of stat error or human error, so if you find any conclusion with your studies, cross check it with reality, talk with someone experienced with context and talk with locals. Hope this helps with your sampling operation in humanitarian studies! Best!

##   Modality of Surveys

Personally, I can not go into details of this section. This concept is directly linked with context. Yet, there are some points that I experienced. Basically, if you have the opportunity of phone surveys, amazing. They are cheap and fast. If you have a target group scattered wide geographical areas, face to face surveys are hard to conduct and very expensive. 

In my concept, we are doing PDMs via phone and there is a study called IVS (intersectoral vulnerability study) helds by house visits. In context of ESSN, we do have capacity to conduct face to face surveys, yet, our target group is all around Türkiye. So if you do a random sampling, you may end up someone with 300 KM away in a village, just one person to visit there. Not a good plan for operation. Your field staff most likely will not even visit that village. What we did to prevent that is we see that 10 provinces covers %95 of our target group, out of 81 provinces. So we focus on these ones to make study more budget-operational friendly. Also, we reduced confidence level from 95 to 90. So there was a trade off of losing some information of population, but having more in-depth information of others. Because, face to face surveys gives you more room to study with. You can observe household assets, data quality is better and your FMAs (field monitoring assistants) can observe vulnerability of the households. For phone surveys, as we mentioned, they must be short and understandable. One cannot go into detailed complex questionnaires with that modality.

Selected survey modality affects questionnaire design step, so first thing is to decide the survey modality and than decide indicators of logframe and their means of verification (data source). To decide modality, you need to know operational capacity and budget- with an understanding of geographical coverage of the programme and target group characteristics.

Also there are different methods of sampling useful for face to face surveys. I would recommend to check those methods if you are to organize F2F modality; check Annex I -Sampling of CVME held by WFP. (https://docs.wfp.org/api/documents/WFP-0000112161/download/)




















