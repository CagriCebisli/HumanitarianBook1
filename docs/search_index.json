[["index.html", "Data Science Within Monitoring and Evaluation in Humanitarian Context Chapter 1 Introduction 1.1 Audiance 1.2 Why Is This a Book Needed? 1.3 Book Structure", " Data Science Within Monitoring and Evaluation in Humanitarian Context Cagri Cebisli 2022-12-21 Chapter 1 Introduction This book is tailored for revealing key concepts of intersections between “data science” and “monitoring and evaluation” for humanitarian sector. Do not question the language since most of these thoughts and conclusions are personal, and this book acts as a notebook of my experiences, also, I am horrible at writing stuff down. No shame, that is the golden rule of being monitoring and evaluation person. Shame triggers emotions, and as an outcome one hides stuff and reflects only bright side, and that reduces lessons learned capacity for sure. Five years in humanitarian sector as data scientist with background of industrial engineering and management information systems, I have encountered gaps and common mistakes in the sector. For some parts, I will be questioning these gaps and unknowns for building a collective knowledge, hoping that it will help the future actions. Through the book I will take a guess that reader have basic knowledge of monitoring and evaluation in humanitarian sector so I will dodge going in-depth of the aspect, and the book will focus on data analysis and tips&amp;tricks of designing operational planning. Humanitarian sector actors has background of “monitoring and evaluation”. There are many individuals whom got mindset and experiences towards this aspect. Yet, in the latest versions of M&amp;E, inevitably this aspects merges with data and knowledge. One of the most experienced individuals with humanitarian data Aldo Benini reflects in his notes; “Keep it simple. You may use R or Python, but if you want to be a voice for this spectacular community, stay with excel, since most of the people are more relaxed using this.” This conclusion of years of experience is still valid. Yet, I am observing two types of individuals. On one hand you have mid-class proficient excel users, whom can conduct (descriptive) analysis. On the other hand one has very skilled individuals whom can use R or Python with their amazing machine learning skills and so on. Thus, in the sector, we do not have one common ground but two. As for data analysis for humanitarian sector, with the motivation of donors, we are observing more “evidence-based” actions. One of the old-school WFP manager once told in a VAM training: “Back than we were disturbing in-kinds as food, hygiene whatsoever, and then come back home. Nobody was asking question neither we did care for it. We believed we did good and that’s all. But now, there is data everywhere and logic is more stronger.” This is a clear vision of experience, telling us how data become more important during years. For red pillar, IFRC is pushing more for IM - when they are losing sight of M&amp;E analysts and mark this concept as PMER where these individuals mostly have no idea about data analysis. So it looks like quantitative staff is IM and qualitative staff is PMER, which is a terrible idea. A data scientist can become IM, but can not become M&amp;E data analyst, since they have to have an experience for programme design-logframes-impact analysis and understanding concept of the programme. Frankly, we did have an issues with this mindset while working with IFRC. M&amp;E Data Scientist and IM persons are not the same thing. Through the book, one will realize this more. ICRC does cover a different path which I would prefer as well. They call this phenomenon as “Analysis and Evidence Team”. Which does serve their structure well, as they are more enveloped and their work-groups are focusing on very different topics. Yet, they have intersections such as between Protection(unit) - ECOSEC or ECOSEC (economic security) - WATHAB (water habitat). But mentality of the unit is the same. It is like having a team that makes knowledge out of ones data or any secondary data. That team creates room for being more agile, moving with evidence, not with instincts (tho sometimes it’s better, we do not make data fetish here). Their team can be interpreted as M&amp;E Data Analyst team, under covered. If you read their documents, they are pretty similar with any other M&amp;E guidelines and handbooks. For some programme coordinators (usually happens in National Societies) I am seeing the attitude of taking data analysis, evidence-based decision making and M&amp;E is a layer of quality of a programme instead of considering M&amp;E as core foundation of any humanitarian programming which should shape other operational/programmatic components. This mindset is an eclipse in humanitarian sector. These thoughts are belongs to past, what WFP manager refers to. This eclipsed-mind still lives in 50 years ago, where only aim of a programme is to deliver assistance, without any monitoring, without understanding needs, without any targeting, without any reporting at all. If you see an environment and attitude like that, just leave the ship. Donors will not approve that either. For someone who worked for ESSN, biggest cash programme of ECHO and in humanitarian sector, I have an understanding of how ECHO interprets monitoring and what they expect out of evaluation studies. ECHO does care about evidence-based management style, and M&amp;E. They do respect their logframes. Thus, if one puts an indicator there can expect a set of questions towards its interpretation. Also in interim reports they are very interested in calculation of MEB, how indicators talks with each other and so on. So if you are an M&amp;E data analyst with ECHO supported programme, expect series of questions and prepare good story-telling 3-pagers. It will be a challenge! I strongly recommend to read out DG Echo Thematic Policy Document No 3 - Cash Transfers, March 2022. Personally, I am not experienced with blue pillar (UN). Yet WFP is leading this concept and they are disaggregating more into “division of labor”* doctrine. They have monitoring unit, sided by VAM unit (vulnerability analysis and mapping). Even there is a unit collaborated by UNHCR and WFP, just for targeting. Also they have an unit studies targeting. Worked with them for targeting exercises for camps, and I learned a lot from them! No wonder why they are in a leading role, as they care and dig more into these lands. Most of the red-pillar managers does not understand what targeting is, yet other hand has an unit for it. As mentioned, personally, I do have very little information of other UN institutions. Yet, WFP is a shining role-model for sure. This is a one, dusty overlook to humanitarian sector and data. I am smelling a trend between cash based interventions and data-knowledge tree. Huge correlation is lurking, if you check ECHOs latest Large-scale cash programme guidelines, lots on monitoring and evaluation explained. Thus, if humanitarian sector evolves more into large-scale cash, data and M&amp;E will become more and more important. In this book, I will not dig into if it is possible to do more cash-based interventions and infrastructure of banks or stakeholders’ potential. Yet we all can agree that cash still has a big future and a room in our daily lives as humanitarians and Ukraine crisis proves that right. So lets build more towards data analysis and M&amp;E to increase accountability, evidence-based actions and to have better programmes. Note that in this paper, I am assuming that reader has basic understanding of M&amp;E, and stresses more on the data dimension. A small note on division of labor, which is an important aspect. Because, if we go all in with specialized skills, we have to divide tasks into small piece and assign them to specialists.. Division of labor, the separation of a work process into a number of tasks, with each task performed by a separate person or group of persons. It is most often applied to systems of mass production and is one of the basic organizing principles of the assembly line. Considering workload and standard reporting of the unit, current flow can be interpreted as assembly line. Pros for division of labor in unit: Efficient mastery Quicker training Considering unit will hire more people and circulation of individuals in sector, this is a very important key point. Productivity Innovation Cons for division of labor in unit: Boredom of repetition Roles will not assign to each site and each site will work horizontally. Thus we will eliminate this con. Interdependence Not a con for our unit. Collective work is a must. Lack of responsibility Better carrier chances and additional studies will eliminate this con. Some possible team units in the future: IMER (Information management, monitoring, evaluation, reporting) VANA (Vulnerability and Needs Assessment Unit) MER (Monitoring, evaluation, reporting –We have PMER but planning should be separate set of skills and title) 1.1 Audiance The primary audience for this book is data science / data analyst practitioners whom works for monitoring and evaluation units, from organisations directly involved in the design, implementation, monitoring, and accountability of projects using cash and vouchers to deliver humanitarian relief. For example: Monitoring and evaluation data analysts PMER staff with data skills IM teams involved in humanitarian data analysis MEAL specialists etc. The secondary intended audience is other humanitarian stakeholders involved in advancing CTP monitoring and evaluation practices. Thus, may include TPM institutions as well. 1.2 Why Is This a Book Needed? As mentioned in introduction, data analysis within monitoring and evaluation is a growing aspect. Many new colleagues are taking part, some of them has skills towards data analysis and some grades up from field teams, some are just new to the sector or to data analysis. In this book, I will mention about M&amp;E aspect and give some examples through operation, data cleaning, data analysis and basic examples of calculating humanitarian indicators (few important ones usually involves with cash-based interventions) with R. Thus, my belief is to make this book an honest starter for my colleagues, to read and learn from each other. Having no other hands-on guidance and written experiences was missing for me, so I am trying to fill that gap. Of course, every institution has their SOPs, but these SOPs are usually serves and limited with their current context. 1.3 Book Structure This book mainly runs in two pillars; i) Monitoring and evaluation design in humanitarian sector ii) Data Science methods. In this book, for every section, I will set a scenario, a scene to reflect reality and then will take action with coding and explain how to implement studies to a real life situation. Data sets will be random, the ones comes with packages. Anyhow, for different exercises I will be using random data sets from some packages but main concept will be same with humanitarian data. Here, mentality of tidy data comes into play, where every row stands for analysis unit and every column stands for variables. Structure of the Book "],["humanitarian-projects-and-monitoring-and-evaluation.html", "Chapter 2 Humanitarian Projects and Monitoring and Evaluation", " Chapter 2 Humanitarian Projects and Monitoring and Evaluation Before moving forward, a basic set of project-monitoring definitions could be useful. There are several types of monitoring in any programme such as; process monitoring, results monitoring, context monitoring, risk monitoring, impact monitoring etc. Monitoring4CTP, a monitoring guidance for CTP made by USAid and CALP, is a fruitful and well-tailored document. I strongly advise this document to anyone who works in humanitarian sector. Project Monitoring by CALP As per monitoring type, there are several method intersections. Yet, the data skills comes into play when we ask evaluation questions. An amazing visual prepared by IFRC, we can clearly see those steps and questions. Project Monitoring by CALP Yes, all good, but why M&amp;E is important ? Support project/programme implementation with accurate, evidence-based reporting that informs management and decision-making to guide and improve project/programme performance. Contribute to organizational learning and knowledge sharing by reflecting upon and sharing experiences and lessons so that we can gain the full benefit from what we do and how we do it. Uphold accountability and compliance by demonstrating whether or not our work has been carried out as agreed and in compliance with established standards. Provide opportunities for stakeholder feedback, especially beneficiaries, to provide input into and perceptions of our work, modelling openness to criticism, and willingness to learn from experiences and to adapt to changing needs. Promote and celebrate our work by highlighting our accomplishments and achievements, building morale and contributing to resource mobilization These points are quoted from M&amp;E guide of IFRC. All smooth right? Did you ever see a humanitarian programme, especially a cash-based intervention with zero monitoring? I did and it was horrible. No accountability, no promotion and celebration of work, no highlights, no lessons learned, no voice of beneficiaries. Just distribute some cash to undefined and unjustified target group and come back home. Still think a programme without M&amp;E or AAP/CEA is a good idea? Still think that this is a quality layer and not a must? You will not survive in humanitarian sector. Imagine you are a donor, would give your assets to someone with good M&amp;E system and monitoring plan or would you pick other way around. "],["research-design-for-monitoring-and-evaluation-studies.html", "Chapter 3 Research Design for Monitoring and Evaluation Studies 3.1 Logframes, Indicators and Questionnaire Design 3.2 Data Collection Tools 3.3 Sample 3.4 Modality of Surveys", " Chapter 3 Research Design for Monitoring and Evaluation Studies In humanitarian sector research design is actually links with monitoring and evaluation plan of the programme. Through all different style of M&amp;E plans (actually can be called as research design of making programmes accountable and measurable), there are several topics that I see as a must. An introduction What is happening in context and objective of the programme If secondary data available, most changing elements like a brief summary of markets etc. Purpose of the plan By implementing this plan, which questions we will answer? (Is programme impactful as intended? could be a research question for M&amp;E) Why we are doing M&amp;E and where are the bottlenecks Expected outcomes and reporting Description of the process, which activities will be implement? Example of activities; FGD, PDMs, KIIs, Needs Assessments etc. Scope of each activity (disaggregated by activity) How data will collect? What will be modality of data collection. Requirements on the field. Tools needed to conduct activities. How many man/work hours do we need? What will be the frequency of reports and what/how to report? Data flows What is the data collection tool, KOBO/ODK etc? Where is the database? Data security 3.1 Logframes, Indicators and Questionnaire Design Through logframe, there will be several indicators that is measured by M&amp;E teams. Team must tailor surveys to collect these indicators. There are several different methods of reflecting a log frame, yet, the one I propose is below. This version reflects anything needed; a description of indicator, data source, baseline, target, data source, frequency, main responsible, and most importantly, assumption and risks. Logframe overlook, kindly zoom in to read, sorry! Lets say, you are an M&amp;E specialist to a humanitarian programme. Donor asks you to use SMART indicators (please do google it, very useful topic). Then, with the blessings of donor, or in your proposal to a donor, you reflect that your indicators will be rCSI and FCS. Second step is to designing your PDMs. rCSI is an indicator that builds over 5 questions. Thus, you have to have these questions to calculate and report rCSI. We will dig more into these indicators, how to calculate them with R in the following sessions. In a nutshell, questionnaire design orients around logframe indicators. Simple as that. Calculation of rCSI Additional parts of questionnaire design comes from analyst. If analyst wants to conduct a vulnerability study depending on multi-dimensional aspect, that includes household assets, these can be added as wished. So any metric compounds of set of questions can be added to the questionnaire. There are two things analyst must be careful about; Data collection modality. If modality is via phone, surveys must be short, respecting data quality. Questions must be simple and easy to collect via phone. Even for the face to face surveys, length of questionnaire is an important aspect. Communication must be clear and questions must not be complicated or open to bias. Every question must serve to a purpose. Avoid unnecessary questions or any open-ended ones for quantitative study. 3.2 Data Collection Tools There are several different tools to collect data. IFRC generally uses KOBO. If you google, there are two links for KOBO. IFRC one called https://kobonew.ifrc.org/ and other one is https://www.kobotoolbox.org/. As far as I am aware Kobotoolbox is under OCHA’s administration. But why we have two different KOBO? That is because of servers. IFRC servers in Frankfurt, OCHA based in US. Thus, your data copyrights are protected by those laws. https://humanitarian.atlassian.net/wiki/spaces/imtoolbox/pages/3190489103/Kobo+Toolbox+Terms+of+Service: U.S. DIGITAL MILLENNIUM COPYRIGHT ACT. If UNOCHA receives a notice alleging that material from your account infringes another party’s copyright, UNOCHA may disable your user account or remove alleged material in accordance with Title II of the Digital Millennium Copyright Act of 1998 (Section 512 of the U.S. Copyright Act). ICRC uses “Device Magic” and they have their own servers. Turkish Red Crescent uses different elements, in ESSN they are using ODK and servers are based in Turkey (admin is Turkish Red Crescent) due to data security. So there are several different means of data collection tools. My perspective, I find KOBO very efficient, but there is always a question of data security if you don’t own the server. In this book, I will dodge how to use those platforms. Yet, they are pretty simple and KOBO even has UI to do it. Took me two nights to completely understand how it works. Also, there are several training available online. 3.3 Sample Taken from “IndiKit Rapid Guide to Survey Sampling”, an amazing guide for humanitarians. Sample is basically choosing who to survey. First, define your target population – the people we are interested in collecting data from. The most common target populations are: people whom your intervention aims to help (its direct beneficiaries) other people who might have benefited indirectly (can be used for assessing the intervention’s spillover effect) other stakeholders of your interventions whose opinions and other data you need to assess members of your comparison group various population groups living in a given area (commonly used for needs assessments) Keep in mind that one survey can have several different target populations. For example, a baseline survey of a nutrition-sensitive agricultural intervention might target children under 5 years (for measuring the prevalence of under nutrition), their caregivers, trained farmers as well as agriculture extension workers. Such surveys then require separate samples for each target group. Please kindly refer to document mentioned above. Here, I will be giving some tips and tricks from my personal experiences and try to reflect how I do things operationally, in a most simplified way. Sampling Roadmap 3.3.1 Analysis Unit While building logframe, indicators are mostly orients around household level. Like copings, food security, expenditures etc. Thus, most of the time in humanitarian projects “analysis unit” is households. Some cases could be individual such as disability projects, education etc. Your target group defines perspective of the project. In a programme, if your registration is at household level, automatically analysis unit will be in household. 3.3.2 Non-Response Rates Non-response rates are very important. This means a sample unit from your list that operators or FMAs cannot reach. If you have to reach 385 households in a region and have a list of 400, with %50 non-response rate, your list will not be enough for representative number. You need at least 385 x 2 = 770 sample to reach desired number of surveys. 3.3.3 Camp Context In camp context, most easy-to-apply method is following; Give numbers to each containers and convert it to a vector and shuffle it. Generate random numbers from that vector by order. Pick intersection of two vector as sample. There you have it. A random selection by containers for sampling. You may need satellite image to see the number of containers and where they are settled. If approved, a drone may do as well. Attention is required for analysis unit-container relationship. If your analysis unit is households, than your sampling unit must be as well. Yet, imagine a concept where containers are not obtained by per household. Than, different set of approach is required. Thus, effecting questionnaire design. 3.3.4 Drawing Sample with R 3.3.4.1 Layered-Clustered Sample In humanitarian sector, I see that programme monitors usually applies clustered samples. Cluster layers could be regions, nationality, gender etc. where researchers wants to have a representative knowledge for each cluster group. There are several different methods for sampling. I personally like machine learning style of sampling. library(tidyverse) library(ggplot2) head(diamonds,5) ## # A tibble: 5 × 11 ## carat cut color clarity depth table price x y z Type ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 Population ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 Population ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 Population ## 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 Population ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 Population Lets do this exercise over “Diamonds” data comes with tidyverse package. Thus, this data is population data and you need to draw some samples for a study. Setting the Scene: You are a data scientist works with monitoring and evaluation unit in country office x. There is a cash-based intervention and you are responsible for designing a PDM. In programme objective, it says; increasing economic situation of the target population. So this is basically your research question. Now it is time to build a research design. In secondary data review, you find out that for specific regions, socio-economical indicators are very different (livelihoods and expenditures etc.). Thus, to have an accurate outcomes, you design a representative sample for each region, to have a better understanding of the concept. Operationally, you have enough capacity to collect data within promised time (I am stating that because dividing sample into layers means more surveys, it could be overall 385 or for two regions 385 x 2). Now, you have the population data and sample stage is at hand. In first version, we will draw a quick sample from overall population and then move to more simplified version of layered sample. #A quick overlook to data glimpse(diamonds) ## Rows: 53,940 ## Columns: 11 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.30, 0.23, 0.22, 0.31, 0.20, 0.32, 0.30, 0.30, 0.30, 0.30, 0.30, 0.2… ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Very Good, Fair, Very Good, Good, Ideal, Premium, Ideal, Premium, Prem… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I, E, H, J, J, G, I, J, D, F, F, F, E, E, D, F, E, H, D, I, I, J, D, … ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, SI1, SI2, SI2, I1, SI2, SI1, SI1, SI1, SI2, VS2, VS1, SI1, SI1, VVS… ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64.0, 62.8, 60.4, 62.2, 60.2, 60.9, 62.0, 63.4, 63.8, 62.7, 63.3, 63.… ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58, 54, 54, 56, 59, 56, 55, 57, 62, 62, 58, 57, 57, 61, 57, 57, 57, 5… ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 342, 344, 345, 345, 348, 351, 351, 351, 351, 352, 353, 353, 353, 354,… ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.25, 3.93, 3.88, 4.35, 3.79, 4.38, 4.31, 4.23, 4.23, 4.21, 4.26, 3.8… ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.28, 3.90, 3.84, 4.37, 3.75, 4.42, 4.34, 4.29, 4.26, 4.27, 4.30, 3.9… ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.73, 2.46, 2.33, 2.71, 2.27, 2.68, 2.68, 2.70, 2.71, 2.66, 2.71, 2.4… ## $ Type &lt;chr&gt; &quot;Population&quot;, &quot;Population&quot;, &quot;Population&quot;, &quot;Population&quot;, &quot;Population&quot;, &quot;Population&quot;, &quot;Population&quot;, &quot;Population&quot;, &quot;Population&quot;, &quot;Po… Lets take “cut” as region here. This variable is categorical with 5 unique values. There are several good-ends of taking sample with data partition. More info can be found in: http://topepo.github.io/caret/data-splitting.html If you need to calculate threshold (desired number for representative sample), Raosoft (http://www.raosoft.com/samplesize.html) is a good sample size calculator. So in this example, lets decide metrics wih Raosoft; with %5 margin of error, %95 confidence level, population size of 53940 and %50 response distribution (maximum unknown) our sample size is 382. Including %50 non-response rate sample size (a call list or interviewee list) is 764. In the current situation we are using cluster sampling so that for five different categories (could be region, nationality etc. in humanitarian sector) we need to draw 382 (764) for each of them. We did take ‘cut’ as our layer in the sampling. Thus, for five categories we need sample of 764 * 5 = 3820 sample which is ~%7.1 of our population. #Easiest way to draw layered sample. One row coding, amazing package! layered_sample &lt;- diamonds %&gt;% group_by(cut) %&gt;% slice_sample(n=780,replace = FALSE) table(layered_sample$cut) ## ## Fair Good Very Good Premium Ideal ## 780 780 780 780 780 Now that we draw sample. But, we have to compare population metrics with sample metrics. As stressed in scenario paragraph, we are trying to understand socio-economic variables from sample, and then we will assume these findings applies to whole population. A small trick, we might forecast how successful our sampling in terms of making conclusions. We do have an information about our sample; lets say household size. So if your sample household size distribution and your population is not statistically different, we can assume that our sample does well and outcomes of socio-economic conclusions are applies to whole population. Lets start with categorical variables. Below, from graphs, one can see that distribution of clarity and type (Remember these are coming from diamonds data. You can interpret them as any other categorical variable that is available in humanitarian sector). What this visual tells us is this: density of group taken into consideration and distributions applies to whole case, without leaving any different characteristic behind. So that is a good sample! diamonds$Type &lt;- &#39;Population&#39; layered_sample$Type &lt;- &#39;Sampled&#39; Compare &lt;- rbind(layered_sample,diamonds) ggplot(Compare, aes(clarity, Type)) + geom_jitter(aes(color = clarity), size = 0.3)+ ggpubr::color_palette(&quot;jco&quot;)+ ggpubr::theme_pubclean() Let’s double check with numeric variables and compare them by type. Depth and Table variables taken into consider. Check how both linear models does not aligns with each other perfectly, but does well enough. Here, we can assume that our sample does really well! library(hrbrthemes) Compare %&gt;% ggplot(aes(x = depth, y = table, group = Type, color = Type)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Variable Table&quot;, y = &quot;Variable depth&quot;) + theme_ipsum() Key differences between sample and population happens due to we give same importance to each cut category. But in reality, their distribution (their frequency-density in population) is different, yet we did draw 780 sample from each of them. This is where one must apply weights in order to balance the outcomes and to have better accuracy while making assumptions of population values. Weights can be calculated as: each categories’ sample size ratio (# of survey of each category divided by total sample-survey number) divided by each categories’ population size ratio (frequency of each category divided by total population number) will give you weights. Calculating Sample Weights 3.3.4.2 Random Sampling For random sampling, without any clusters, data partition technique is amazing. Lets say we want to draw 3820 sample out of 53940 (row number of diamonds data), which is ~7.1% of our total population. “p” metric in data goes as 0.71. We mark selected cases as index, and split data by sampled and outsampled. Distribution here will be more accurate then layered sampling. Please do check linear line and table of cut variable. #Select rows - proportionally depending on &quot;cut&quot;~ acts as region. #library caret contains &quot;createDataPartition&quot; function. library(caret) diamonds_index &lt;- createDataPartition(diamonds$cut, p = .071, list = FALSE, times = 1) #Divide data by sample: selected -&gt; sampled &amp; Not in sample -&gt; outsample diamonds_sample &lt;- diamonds[ diamonds_index,] diamonds_outsample &lt;- diamonds[-diamonds_index,] #As you see, frequency of cut category is not same with clustered version. #Percentage of each cut category is same with population percentages now. table(diamonds_sample$cut) ## ## Fair Good Very Good Premium Ideal ## 115 349 858 980 1531 library(hrbrthemes) diamonds$Type &lt;- &#39;Population&#39; diamonds_sample$Type &lt;- &#39;Sampled&#39; Compare_partition &lt;- rbind(diamonds_sample,diamonds) Compare_partition %&gt;% ggplot(aes(x = depth, y = table, group = Type, color = Type)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Variable Table&quot;, y = &quot;Variable depth&quot;) + theme_ipsum() You can see that linear models are overlaps better, if you compare it to clustered sample. This is also another good example of why we need to apply weights in clustered samples, before making an assumption towards population. 3.3.4.3 Lessons Learned from Sampling I would also like to reflect few lessons learned from my end towards sampling-operational balance. If you cluster your sample with regions, select proper enumerator for proper regions. In some cases, some regions may speak different language, or attendees may be more coherent when enumerator speaks their language or when enumerators are from same region. Selecting an enumerator from or experience with that region usually reduces survey time and increases data quality. Also they are better when you ask for feedback towards questionnaire. Take into consideration of gender of your enumerator, where in some regions female enumerators can have positive added values while surveying. Keep in touch with enumerators, if they feel something is wrong with sampling, let them warn you. For ex; all the attendees saying that they are living in some other region than your sampling list-call list. Keep track of enumerators if you have clustered sample. Lets say you are calculating %50 of non-response rate of 300 survey, thus your sampling is 600. For some clusters non response rate might be lower, so they may exceed the number of 300, means that you have to stop those enumerators to do extra calls. But of course, always have some extra calls since you might delete some records during data cleaning. And vice-versa, some clusters may have non response rate more than %50, so you might need to give them extra call list during operation. When you share your coding, do not forget to add seeds into your coding, so that your studies can be reproducible by others. When you do random sampling over population data, if you do care a bit about some categories as well (like region, sex, nationality etc.), their frequency might be low like 30~ish in your sample. Round them up to at least 50-60 to get an idea about those disaggregations so that you can conduct some t-test etc. This marks the end of our sampling section. Always remember, we need good sampling to make accurate decisions. There is a chance of stat error or human error, so if you find any conclusion with your studies, cross check it with reality, talk with someone experienced with context and talk with locals. Hope this helps with your sampling operation in humanitarian studies! Best! 3.4 Modality of Surveys Personally, I can not go into details of this section. This concept is directly linked with context. Yet, there are some points that I experienced. Basically, if you have the opportunity of phone surveys, amazing. They are cheap and fast. If you have a target group scattered wide geographical areas, face to face surveys are hard to conduct and very expensive. In my concept, we are doing PDMs via phone and there is a study called IVS (intersectoral vulnerability study) helds by house visits. In context of ESSN, we do have capacity to conduct face to face surveys, yet, our target group is all around Türkiye. So if you do a random sampling, you may end up someone with 300 KM away in a village, just one person to visit there. Not a good plan for operation. Your field staff most likely will not even visit that village. What we did to prevent that is we see that 10 provinces covers %95 of our target group, out of 81 provinces. So we focus on these ones to make study more budget-operational friendly. Also, we reduced confidence level from 95 to 90. So there was a trade off of losing some information of population, but having more in-depth information of others. Because, face to face surveys gives you more room to study with. You can observe household assets, data quality is better and your FMAs (field monitoring assistants) can observe vulnerability of the households. For phone surveys, as we mentioned, they must be short and understandable. One cannot go into detailed complex questionnaires with that modality. Selected survey modality affects questionnaire design step, so first thing is to decide the survey modality and than decide indicators of logframe and their means of verification (data source). To decide modality, you need to know operational capacity and budget- with an understanding of geographical coverage of the programme and target group characteristics. Also there are different methods of sampling useful for face to face surveys. I would recommend to check those methods if you are to organize F2F modality; check Annex I -Sampling of CVME held by WFP. (https://docs.wfp.org/api/documents/WFP-0000112161/download/) "],["descriptive---primelinary-analysis-exploratory-data-analysis---eda.html", "Chapter 4 Descriptive - Primelinary Analysis (Exploratory Data Analysis - EDA) 4.1 Motivation 4.2 Data Cleaning 4.3 Exploratory Data Analysis (EDA) 4.4 Designing the Logframe - Calculation of Exampled Indicators", " Chapter 4 Descriptive - Primelinary Analysis (Exploratory Data Analysis - EDA) 4.1 Motivation To make sure data cleaning step is automatized and consistent. To make sure data contains relevant variables before analysis. To make sure there are no unexpected variable type issues. Detect any missing values, especially in numeric columns to understand if it should be NA or 0. Control and compare different users` values for specific variables. Calculate must-have indicators such as LCSI and RCSI. Make data TIDY… Such as: multiple selection questions are in dummy variables and binary now. Setting the scene for having reproducible studies. 4.2 Data Cleaning 4.2.1 Column Renaming If there is an error on this part, check error details. Following issues might be in-place; Duplication on column names Removal of some columns from the original data Basically, if the following code and imported data sets column names does not match, this error is inevitable. For solution steps: Check error message, find out which columns have error. Correct the column names using chunk of coding below by accurate names. #Code example; I am using rename function but there might be better ones for sure. #Lets keep using diamonds data for this as well. #rename comes with dplyr. library(dplyr) library(tidyverse) DiamondsRenamed &lt;- diamonds %&gt;% rename(&quot;renamed_table&quot;=&quot;table&quot;) colnames(diamonds) ## [1] &quot;carat&quot; &quot;cut&quot; &quot;color&quot; &quot;clarity&quot; &quot;depth&quot; &quot;table&quot; &quot;price&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; &quot;Type&quot; colnames(DiamondsRenamed) ## [1] &quot;carat&quot; &quot;cut&quot; &quot;color&quot; &quot;clarity&quot; &quot;depth&quot; &quot;renamed_table&quot; &quot;price&quot; &quot;x&quot; ## [9] &quot;y&quot; &quot;z&quot; &quot;Type&quot; 4.2.2 Converting Multiple Selections into Dummy Variables This issue happens with ODK or Device Magic, as they put multiple selections question into one variable. KOBO does this dummy coding it self, which is amazing. Converting these columns might consume a bit of time, let me introduce you how I manage this bothering situation… If you have a better way to do it, let me know! #Lets add a column to diamonds to work with #We added multiple selections divided by comma DiamondsRenamed$MultSelec &lt;- &quot;Selection1,Selection2,Selection5&quot; #Take this column as data frame from our main data frame df_multselec &lt;- as.data.frame(DiamondsRenamed$MultSelec) #Rename this data frame column colnames(df_multselec)&lt;- &quot;examplecolumn&quot; #We will use cSplit_e function within splitstackshape package library(splitstackshape) #Now you have these columns as dummy ones, if it is selected it goes as 1 otherwise as 0. #Each category becomes a column with binary format. df_multselec_seperated &lt;- cSplit_e(df_multselec, split.col = &quot;examplecolumn&quot;, sep = &quot;,&quot;, type = &quot;character&quot;, mode = &quot;binary&quot;, fixed = TRUE, fill = 0) #Remove original column before merging with original data, so that you will not have duplicate columns. df_multselec_seperated$examplecolumn &lt;- NULL #With column bind, add new dummy columns into original data. Done. DiamondsRenamed &lt;- cbind(DiamondsRenamed,df_multselec_seperated) #Lets see what happened. Check last three columns as example column. library(dplyr) glimpse(DiamondsRenamed) ## Rows: 53,940 ## Columns: 15 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.30, 0.23, 0.22, 0.31, 0.20, 0.32, 0.30, 0.30, 0.30… ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Very Good, Fair, Very Good, Good, Ideal, Premium, Ide… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I, E, H, J, J, G, I, J, D, F, F, F, E, E, D, F, E, H… ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, SI1, SI2, SI2, I1, SI2, SI1, SI1, SI1, SI2, VS2, V… ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64.0, 62.8, 60.4, 62.2, 60.2, 60.9, 62.0, 63.4, 63.8… ## $ renamed_table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58, 54, 54, 56, 59, 56, 55, 57, 62, 62, 58, 57, 57, … ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 342, 344, 345, 345, 348, 351, 351, 351, 351, 352, 35… ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.25, 3.93, 3.88, 4.35, 3.79, 4.38, 4.31, 4.23, 4.23… ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.28, 3.90, 3.84, 4.37, 3.75, 4.42, 4.34, 4.29, 4.26… ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.73, 2.46, 2.33, 2.71, 2.27, 2.68, 2.68, 2.70, 2.71… ## $ Type &lt;chr&gt; &quot;Population&quot;, &quot;Population&quot;, &quot;Population&quot;, &quot;Population&quot;, &quot;Population&quot;, &quot;Population&quot;, &quot;Population&quot;, &quot;Population&quot;, … ## $ MultSelec &lt;chr&gt; &quot;Selection1,Selection2,Selection5&quot;, &quot;Selection1,Selection2,Selection5&quot;, &quot;Selection1,Selection2,Selection5&quot;, &quot;Sel… ## $ examplecolumn_Selection1 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ examplecolumn_Selection2 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ examplecolumn_Selection5 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… 4.2.3 Missing Values 4.2.3.1 Defining Missing Values of Dataframe Usually we do not have missing values, if questionnaire design tailored well in Kobo or ODK. Conditional or mandatory questions can be defined during this phase. Still, checking missing data always a must. Data cleaning step should be held with consultation of whoever design KOBO-ODK forms. These persons can explain you tips and tricks of the data and could give you a parade of the data. #Required packages are visdat and naniar. library(visdat) library(naniar) #Lets see if diamonds data have some missing values here and there. vis_dat(diamonds) There are no missing values in diamonds data. One of the amazing thing of this visual is it gives you types and structures of the each variable. This thing sure is an art! Amazing! Lets remove some (put some NA values intentionally) values and see that amazing visual again. #Required packages for adding random NAs to data is missMethods. library(missMethods) #With delete_MCAR function, we delete random values to 25% of the data and make them NA. diamonds_missing &lt;- delete_MCAR(diamonds, p = 0.25) vis_dat(diamonds_missing) So now, grey lines in the visual reflects NAs. Amazing! Lets check the percentage of missing now: #Percentage of missing by variable. vis_miss(diamonds_missing) Amazing. delete_MCAR function is king. You can see %25 missing randomly from each column. To have a better knowledge about these operations please visit: https://cran.r-project.org/web/packages/naniar/vignettes/getting-started-w-naniar.html Missing frequency. gg_miss_var(diamonds_missing) + theme_bw() Missing frequency disaggregated by categorical variable. gg_miss_var(diamonds_missing, facet = cut) 4.2.3.2 Imputation of Missing Values There are several methods for imputation such as median-mean imputations or one can go all in and use some regressions even. Most of the cases, for some numeric values, you need to replace NAs with zeros in humanitarian data. So lets say, questionnaire designer linked two questions; do you have income?-Yes or no question ; if yes, how much? If the answer is NO, then how much question is not appearing in data, means that this value will be NA. On the other hand, for analysis purposes, you need to embed zero to those values. Happens a lot! Let me show you how I handle this in R (also adding few useful codes that I use during data cleaning); #Lets do this operation for only numeric columns of diamnonds data. #setnafill comes with data.table package. Mapping numeric columns map_lgl comes with purrr package. library(purrr) library(data.table) diamonds_missing[ , purrr::map_lgl(diamonds_missing, is.numeric)] ## # A tibble: 53,940 × 7 ## carat depth table price x y z ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 59.8 61 326 NA 3.84 NA ## 3 0.23 56.9 65 NA NA NA 2.31 ## 4 0.29 NA NA 334 4.2 4.23 NA ## 5 0.31 63.3 NA 335 4.34 4.35 2.75 ## 6 0.24 62.8 NA 336 3.94 3.96 NA ## 7 0.24 62.3 57 336 NA 3.98 NA ## 8 0.26 61.9 55 337 4.07 4.11 2.53 ## 9 0.22 NA 61 NA 3.87 NA 2.49 ## 10 0.23 59.4 61 338 4 NA 2.39 ## # … with 53,930 more rows #Fill NA values with zero only for numeric variables. setnafill(diamonds_missing[ , purrr::map_lgl(diamonds_missing, is.numeric)], fill=0) vis_miss(diamonds_missing) Now, we do not have any missing values for numeric columns as they are converted to (filled with) zeros. Job well done! setnafill is a great piece of function that you can use, please do check it further. 4.2.3.3 Imputation of Missing Values with Regression This method is amazing where you can fill values by categories. Think of humanitarian data and you have some missing values in income. # da1 &lt;- impute_lm(dat, Sepal.Length ~ Sepal.Width + Species) # da2 &lt;- impute_median(da1, Sepal.Length ~ Species) Some useful coding lines; #This piece of code shows you how to select columns starts with etc. #Not related with this concept but I will add those here, very useful piece of code. #CashUsageReplace &lt;- colnames(x1 %&gt;% dplyr::select(ends_with(&quot;_cash&quot;,ignore.case = TRUE))) #EcmenReplace &lt;- colnames(x1 %&gt;% dplyr::select(starts_with(&quot;ecmen_&quot;,ignore.case = TRUE))) #Also, if you want to delete variables, lets say which has missing values greater than %80 of their records, you can use a piece of code given below. #x2 &lt;- x1[colSums(is.na(x1))/nrow(x1) &lt; .8] 4.2.4 Outliers Outliers are more bothering than missing values. Humanitarian data contains outliers but the trick is, you do not know if it is true or not. Lets say you are collecting expenditure data, disaggregated by components such as rent, health, education, celebrations etc. You may end up with a cases like 10000-25000, while mean is 100, median is 150. So these cases are clearly an outlier. Are they? When you contact your enumerators for these type of cases, you may realize some of them are key-entry error, yet some have stories behind. A man might find a debt from somewhere and gets really serious health surgery, or family does a wedding that boost their expenditure towards celebrations. To deal with those cases, I have two copings; Use median if you are to report those values. Medians are more resistant to outliers. If you are doing a simple reporting include them (outliers with stories since they are reflecting truth). If you are doing some sense of regression or any inferential analysis, remove them because they will mess up with your whole algorithm. library(dlookr) library(dplyr) # The mean before and after the imputation of the sodium variable diamonds %&gt;% mutate(price_imp = imputate_outlier(diamonds, price, method = &quot;capping&quot;, no_attrs = FALSE)) %&gt;% group_by(cut) %&gt;% summarise(orig = mean(price, na.rm = TRUE), imputation = mean(price_imp, na.rm = TRUE)) ## # A tibble: 5 × 3 ## cut orig imputation ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Fair 4359. 4258. ## 2 Good 3929. 3823. ## 3 Very Good 3982. 3864. ## 4 Premium 4584. 4414. ## 5 Ideal 3458. 3362. # If the variable of interest is a numerical variable price &lt;- imputate_outlier(diamonds, price) plot(price) 4.3 Exploratory Data Analysis (EDA) There are some really cool packages for EDA in R. These packages-codings below gives really long, boring reports. It goes variable by variable and reflects; Information of variables (n, % missing, unique count etc.) Univariate analysis Distributions, normality test, transformations Comparison of variables by distributions Outlier densities and more… Here is the deal. These outputs are very long consumes time to read it all. But, I strongly recommend creating and reading these reports. If you go somewhere as delegate and your task is to writing a report over monitoring survey data, this exercise allows you to intimate the data, all of the variables and their distributions. Spending an hour with this output will make you to have a way more efficient data analysis step. ExpReport(diamonds, op_file = ‘smarteda.html’) diagnose_report(diamonds) library(dlookr) diamonds %&gt;% eda_report( target = clarity, output_format = &quot;html&quot;, output_file = &quot;EDA_diamonds.html&quot;) Kindly run this code and check output. It will be saved as “EDA_diamonds.html” 4.4 Designing the Logframe - Calculation of Exampled Indicators Through this page, you will see basic logframe indicators of cash-based interventions. As mentioned, these indicators are accepted generally, including ECHO. I will also put documentations about mentioned indicators. Personally, I do love WFP VAM Resource center. I would recommend to check this amazing cookbook as well: https://www.icrc.org/en/publication/4505-economic-security-indicators-cookbook Also, it would be nice to go and check https://www.indikit.net/ where you can find a detailed explanations and examples of most of the humanitarian indicators. Disaggregation of indicators are important. Here, disaggregation only made for resilience capacity score index to show how I personally do it. Visualization of indicators and their codings will be shared in “dashboards” section. 4.4.1 RCSI (Reduced Coping Strategy Index) https://resources.vam.wfp.org/data-analysis/quantitative/resilience/resilience-capacity-score-rcs PAB$rCSI &lt;- PAB$cope_lessexpfood * 1 + PAB$cope_borrowfood * 2 + PAB$cope_reduceportionsize *1 + PAB$cope_reduceadultfood * 3 + PAB$cope_reducemealno *1 4.4.2 FCS (Food Consumption Score) https://resources.vam.wfp.org/data-analysis/quantitative/food-security/food-consumption-score PAB$FCS &lt;- PAB$cons_cereal*2 + PAB$cons_pulses*3 + PAB$cons_veg*1 + PAB$cons_fruit*1 + PAB$cons_meat*4 + PAB$cons_dairy*4 + PAB$cons_sugar*0.5 + PAB$cons_oil*0.5 PAB$FCG &lt;- ifelse(PAB$FCS &lt;=28,&quot;PoorConsumption&quot;, ifelse(PAB$FCS &lt;42, &quot;Borderline&quot;,&quot;AcceptableConsumption&quot;)) prop.table(table(PAB$FCG))*100 ## ## AcceptableConsumption Borderline PoorConsumption ## 43.03109 29.43005 27.53886 4.4.3 LCSI (Livelihood Coping Strategy Index) #Extract livelihoodcoping &lt;- PAB %&gt;% dplyr::select(dplyr::starts_with(&quot;lhood_&quot;)) #Recode vars &lt;- names(livelihoodcoping) livelihoodcoping &lt;- livelihoodcoping %&gt;% mutate(across(all_of(vars), ~ recode(., &quot;No&quot; = 0, &quot;Yes&quot; = 1))) #LCSI Calculation livelihoodcoping$LCSI &lt;- #Stress (livelihoodcoping$lhood_ST_soldhhasset + livelihoodcoping$lhood_ST_spentsavings + livelihoodcoping$lhood_ST_foodcredit + livelihoodcoping$lhood_ST_borrowmoney + livelihoodcoping$lhood_ST_unusualfood ) + #Crisis (livelihoodcoping$lhood_CR_soldprodasset + livelihoodcoping$lhood_CR_noschool + livelihoodcoping$lhood_CR_lesshealth + livelihoodcoping$lhood_CR_lessedu) * 2 + #Emergency (livelihoodcoping$lhood_EM_hhmove + livelihoodcoping$lhood_EM_childwork + livelihoodcoping$lhood_EM_beg +livelihoodcoping$lhood_EM_return) * 3 PAB$LCSI &lt;- livelihoodcoping$LCSI 4.4.4 Resilience Capacity Score https://resources.vam.wfp.org/data-analysis/quantitative/resilience/resilience-capacity-score-rcs 4.4.4.1 Calculation of RCS PAB &lt;- PAB %&gt;% rowwise() %&gt;% mutate( RCS = sum(c(Anticipatory + Absorptive + Transformative + Adaptive + Financial + Social + Institutional + Humancapital + Information))) mean(PAB$RCS) ## [1] 28.98212 4.4.4.2 Normalization of RCS PAB$RCSX &lt;- (((PAB$RCS /9) -1)/ (5-1)) * 100 mean(PAB$RCSX) ## [1] 55.5059 4.4.4.3 Categorization of RCS PAB$RCSG &lt;- ifelse(PAB$RCSX &lt;33,&quot;LOWRCS&quot;, ifelse(PAB$RCSX &lt;66, &quot;MediumRCS&quot;,&quot;HighRCS&quot;)) table(PAB$RCSG) ## ## HighRCS LOWRCS MediumRCS ## 759 74 3027 4.4.4.4 Percentage of Groups prop.table(table(PAB$RCSG)) * 100 ## ## HighRCS LOWRCS MediumRCS ## 19.663212 1.917098 78.419689 4.4.4.5 Disaggregation by Eligibility PAB %&gt;% group_by(ESSN_Status,Control) %&gt;% summarise(mean(RCSX)) ## # A tibble: 4 × 3 ## # Groups: ESSN_Status [2] ## ESSN_Status Control `mean(RCSX)` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Beneficiary Controlz 55.0 ## 2 Beneficiary Testz 55.5 ## 3 Non-Beneficiary Controlz 54.8 ## 4 Non-Beneficiary Testz 56.6 4.4.4.6 Disaggregation by GenderHHH PAB %&gt;% group_by(gender_hh,Control) %&gt;% summarise(mean(RCSX)) ## # A tibble: 4 × 3 ## # Groups: gender_hh [2] ## gender_hh Control `mean(RCSX)` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 female Controlz 54.2 ## 2 female Testz 56.5 ## 3 male Controlz 55.0 ## 4 male Testz 56.0 4.4.4.7 Disaggregation by Stratum PAB %&gt;% group_by(stratum,Control) %&gt;% summarise(mean(RCSX)) ## # A tibble: 10 × 3 ## # Groups: stratum [5] ## stratum Control `mean(RCSX)` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Aegean Controlz 55.1 ## 2 Aegean Testz 56.3 ## 3 Anatolia/Thrace Controlz 53.7 ## 4 Anatolia/Thrace Testz 56.3 ## 5 Istanbul Controlz 55.1 ## 6 Istanbul Testz 56.3 ## 7 Mediterranean Controlz 56.1 ## 8 Mediterranean Testz 55.9 ## 9 South-east Controlz 54.5 ## 10 South-east Testz 55.7 4.4.4.8 Individual Statement Score Calculation a &lt;- colMeans(subset(PAB, select = c(Anticipatory , Absorptive , Transformative , Adaptive , Financial, Social , Institutional ,Humancapital , Information)), na.rm = TRUE) a &lt;- as.matrix(a) colnames(a) &lt;- &quot;AverageValue&quot; a ## AverageValue ## Anticipatory 5.000000 ## Absorptive 2.996632 ## Transformative 3.030829 ## Adaptive 2.964249 ## Financial 2.992487 ## Social 3.024611 ## Institutional 2.984197 ## Humancapital 2.948705 ## Information 3.040415 4.4.4.9 Resilience Capacity Score Spider Map sResilienceCapacities &lt;- PAB %&gt;% group_by(Control) %&gt;% summarise(Anticipatory = mean(Anticipatory), Absorptive = mean(Absorptive ), Transformative = mean(Transformative ), Adaptive = mean(Adaptive )) sRowMax &lt;- c(5,5,5,5) sRowMin &lt;- c(1,1,1,1) sResilienceCapacities &lt;- rbind(sResilienceCapacities, sRowMax, sRowMin) sResilienceCapacities &lt;- as.data.frame(sResilienceCapacities) sResilienceCapacities &lt;- data.frame(sResilienceCapacities[,-1], row.names=sResilienceCapacities[,1]) sResilienceCapacities &lt;- sResilienceCapacities[c(3,4,1,2),] areas &lt;- c(rgb(1, 0, 0, 0.25), rgb(0, 1, 0, 0.25)) # Fill colors areas &lt;- c(rgb(1, 0, 0, 0.25), rgb(0, 1, 0, 0.25)) radarchart(sResilienceCapacities, cglty = 1, # Grid line type cglcol = &quot;gray&quot;, # Grid line color pcol = 2:4, # Color for each line plwd = 2, # Width for each line plty = 1, # Line type for each line pfcol = areas) # Color of the areas 4.4.5 Other SMART Logframe Indicators for CVA Average Total debt / percentage of households with debt MEB based analysis Exclusion - inclusion errors Resilience scores Beneficiary perception positive ratios ECMEN based indicators (please kindly Google this aspect, very important) Check more indicators at: https://www.icrc.org/en/publication/4505-economic-security-indicators-cookbook Check more indicators at: https://www.indikit.net/ "],["creating-a-dashboard-with-r.html", "Chapter 5 Creating a Dashboard with R", " Chapter 5 Creating a Dashboard with R Dashboards are very important communication tools, especially with management end. They like to see numbers and digestible graphs. Personally, selecting correct tool for dashboards are best approach. Two big rivals are in play lately, PowerBI and Tableau. I do personally like PowerBi for simplified dashboards. If your team uses MS Teams, there is an embed option, where you can put your dashboards as tabs within teams channels. Makes life of your manager and related teams easier. R presents few options as shiny or flexdashboard. Flexdashboard is easy to use and learn. Let me quickly introduce you some of my best-in-use graphs. All these data are dummy and made up, so do not focus on making sense, focus on the visualts itself. For me, there are several pillars for cash-based programme monitoring. By order; Income and expenditure Classification (region, nationality etc.) based income and expenditure, if available Log frame programme indicators Indicators by time (if available, if have more than 2 PDMs-data collection) Correlation and Regression analysis of core indicators Transfer value analysis Food security "],["core-pillars-and-codes-of-visuals-of-a-dashboard.html", "Chapter 6 Core Pillars and Codes of Visuals of a Dashboard 6.1 Income and Expenditure 6.2 Income and Expenditure, Disaggregated by Cluster 6.3 Reporting Programme Indicators 6.4 Preparing Outcome-Activity Based X-Table", " Chapter 6 Core Pillars and Codes of Visuals of a Dashboard 6.1 Income and Expenditure Same graph can be made for income as well. Here, one can see the distribution and density levels of the expenditure, reflected by PDMs. Main takeaways will be; If curve is getting skewed towards right, one can assume that expenditure(income) levels are increasing by PDMs. Draw a line on the median point. That will reflect you an understanding of where the middle is. If you are to report, report that line. Tops are important. If curve has two tops, means that there is two group within those expenditure(income) levels. So population is not homogen. If heigh of distribution is getting lower and lower, means that some households are moving from middle-section to lower or upper sections of expenditure(income). More takeaways can be noted with this graph. Amazing visual to understand the levels and story(if have more than two PDms) of the variable. pdmbt %&gt;% filter(x_total &lt; 10000) %&gt;% ggplot(aes(x=x_total, group=PDM, fill=PDM)) + geom_density(adjust=1.5, alpha=.4) + theme_ipsum() + geom_vline(xintercept = median(pdmbt[ pdmbt$PDM==&quot;PDM1&quot; ,]$x_total),color = &quot;red&quot;, size = 1) +labs(x =&quot;Expenditure Levels&quot;, y = &quot;Density Levels&quot;) 6.2 Income and Expenditure, Disaggregated by Cluster Comparing latest two PDMs or latest PDM makes this viz easy to read. Also it does make sense not to go backwards, there should be a limit. As mentioned, last two seems fine. pdmbt %&gt;% filter(PDM==&quot;PDM4&quot; | PDM==&quot;PDM5&quot;) %&gt;% filter(Income &lt; 7500) %&gt;% filter(BenefitSit==&quot;Recipient&quot;) %&gt;% ggplot(aes(x = stratum, y = Income))+ geom_boxplot(aes(fill = stratum), show.legend = FALSE) + geom_density(adjust=1.5, alpha=.4) + theme_ipsum() +labs(x =&quot;Regions&quot;, y = &quot;Income Levels&quot;) + theme(legend.position = &quot;none&quot;) + facet_grid(. ~ PDM)+coord_flip() 6.3 Reporting Programme Indicators Static Reporting For reporting programme log frame indicators, there are two steps. For the first step, report them as success=target, warning=%10 below baseline, danger=%20 below baseline. Report indicators one by one with same method, use flex dashboard, and use page vertically. I am giving an example for FCS. Same coding can be applied to all SMART indicators of log frames. gauge( pi %&gt;% filter(PDMs==&quot;PDM5&quot; &amp; BenefitSituation ==&quot;Recipient&quot;) %&gt;% dplyr::select(FCS) %&gt;% colMeans(na.rm = TRUE) %&gt;% as.vector(), min=0,max=112, gaugeSectors(success = c(63,65), warning = c(41,62), danger = c(0,41)) ) Dynamic Reporting If you have more than one PDM (PAB / baseline) it does makes sense to report this all together, including confidence levels. From graph below you can easily interpret: Trend of indicator. %95 CI of indicator, where it may land in next PDM. Comparison of control and treatment group. pi %&gt;% filter(Programme == &quot;ProgrammeA&quot;) %&gt;% ggplot(aes(x=PDMs, y=Debt, group=BenefitSituation, color=BenefitSituation)) + geom_line() + geom_point() + labs(x = &quot;PDMs&quot;, y = &quot;Debt Held by Target Population&quot;) + theme_ipsum() + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + geom_hline(yintercept=1500, linetype=&quot;dashed&quot;, color = &quot;red&quot;, size=2) + geom_point() + geom_smooth(method = &quot;lm&quot;) + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + geom_text(aes(label=Debt), vjust=0) 6.4 Preparing Outcome-Activity Based X-Table Probably there is a better way to report this, but this is how I do it. If you know a better way to make a Gannt; please do contact me! library(DiagrammeR) #title Monitoring and Evaluation Outcome Indicators mermaid(&quot; gantt dateFormat YYYY-MM-DD section PDMs PDM5+CPDM1 :done, first_1, 2022-06-01, 2022-11-01 PDM6+CPDM2 :active, first_2, 2022-11-01, 2023-01-15 PDM7+CPDM3 : first_3, 2023-01-16, 2023-04-15 section FGDs FGD1, Food Security :crit, done, import_1, 2022-04-01, 2022-05-01 FGD2, Shelter and TV :crit, done, import_2, 2022-08-01, 2022-09-01 FGD3, Topics TBD :crit, import_3, 2022-12-01, 2023-01-01 FGD4, Topics TBD :crit, import_3, 2023-02-01, 2023-03-01 section OSMs On Site Monitoring :active, extras_1, 2022-10-01, 2023-04-01 &quot;) This Gannt is a must to follow-up activities mentioned in M&amp;E plan and programme proposal. I strongly advise to make one during designing M&amp;E plan and costs. This marks the end of this section. Main take aways are: Use flexdashboard or shiny (I don’t know how to use shiny, tho it may be better than flex dashboard) to report indicators, give clear messages and tailor graphs for different audience. I reflected few visuals that I like to use. You can expand this and use different version as well. Carefully track down indicators. They must speak with each other and reflect a full story of what is happening with target group and programme efficiency. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
